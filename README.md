# Bikeshare Event Streaming Pipeline Simulation using Kafka, Spark and DuckDB
 I have used a Kaggle dataset of Capital Bikeshare rides in D.C. from 2020 to 2024, with over 9 million rows, to simulate event streaming using Apache Kafka. I also use Apache Spark to execute distributed processing of big data and I load the data onto my local DuckDB. FInally, i leverage Streamluit to create simulate real-time daily bike ride event streaming with insightful KPIs alongside Random Forest Regression on the daily weather data to forecast ride volume on a day-to-day basis. This allows to provide more granular information to the stakehodlers and decision makers
